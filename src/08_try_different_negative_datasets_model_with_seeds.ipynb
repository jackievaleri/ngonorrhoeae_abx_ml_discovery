{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "146ae400",
   "metadata": {},
   "source": [
    "# TODOs for Krishna\n",
    "- make sure environment works\n",
    "- make sure paths are appropriate for your datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31124c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tqdm\n",
    "from rdkit import Chem, SimDivFilters\n",
    "from rdkit.Chem import DataStructs\n",
    "from sklearn.metrics import auc, precision_recall_curve, roc_auc_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a32a0cdf",
   "metadata": {},
   "source": [
    "# First collect the data we'll be working with and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6256f4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets use the 37K+PK data as our training data\n",
    "df = pd.read_csv(\"../data/data_prep_for_ml/data_prep_for_ml_pk_37k_screen/TRAIN_03_19_2022.csv\")\n",
    "\n",
    "# we'll hold out a random set from here for testing\n",
    "# 20% test set will be sufficient since it is random and class balanced\n",
    "# so for testing, use '../data/TEST_03_19_2022.csv'\n",
    "\n",
    "# we'll also test on all the validation molecules we have predicted on\n",
    "# since their scaffolds are quite different- easy,med,hard,molport\n",
    "# so for testing, use '../data/cleaned_easy_med_hard_val_sets_800k_10_03_2022.csv'\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909a153c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set paths for inputs and outputs\n",
    "out_dir = \"../out/experiment_to_test_different_negative_datasets_seeded\"\n",
    "fig_dir = \"../figure_panels/negative_dataset_example_seeded\"\n",
    "bash_dir = \"experiment_to_test_diff_neg_datasets_train_models_seeded.sh\"\n",
    "models_dir = \"models/experiment_with_diff_neg_datasets_seeded\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f64436a6",
   "metadata": {},
   "source": [
    "# Let's split into sets where we have different proportions of positive:negative data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69c1b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = sum(df[\"hit\"])\n",
    "print(\"num pos: \", pos)\n",
    "\n",
    "num_neg = [pos / 2, pos, pos * 2, pos * 4, pos * 8, pos * 16, len(df) - pos]\n",
    "num_neg = [int(x) for x in num_neg]\n",
    "print(\"number of neg to test:\", num_neg)\n",
    "\n",
    "# preprocess some pos and neg dfs and fp lists for later processing\n",
    "negative_options = df[df[\"hit\"] == 0.0]\n",
    "positive_options = df[df[\"hit\"] == 1.0]\n",
    "\n",
    "all_neg_fps = [Chem.RDKFingerprint(Chem.MolFromSmiles(smi)) for smi in list(negative_options[\"SMILES\"])]\n",
    "all_pos_fps = [Chem.RDKFingerprint(Chem.MolFromSmiles(smi)) for smi in list(positive_options[\"SMILES\"])]\n",
    "\n",
    "# preprocess sorted indices for second method of selection (most similar to positive)\n",
    "tan_sims_from_all_neg_to_pos = [\n",
    "    max(DataStructs.BulkTanimotoSimilarity(query_fp, all_pos_fps)) for query_fp in tqdm.tqdm(all_neg_fps)\n",
    "]\n",
    "sorted_indices_most_similar = np.argsort(\n",
    "    [1.0 - x for x in tan_sims_from_all_neg_to_pos]\n",
    ")  # subtract from 1 so we argsort in most similar to least similar\n",
    "\n",
    "# loop with random seed so we have multiple datasets for each size\n",
    "num_datasets_per_size = 3  # Specify how many datasets you want per size\n",
    "# note that for method 2, you'll always get the same dataset - thats fine\n",
    "for seed in range(num_datasets_per_size):\n",
    "\n",
    "    # Set random seed for reproducibility\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    for i in num_neg:\n",
    "        print(\"testing negative of #: \" + str(i))\n",
    "\n",
    "        # build negative sets in different ways\n",
    "        # First way: random!\n",
    "        rand_neg = negative_options.sample(n=i, replace=False, random_state=seed)\n",
    "        rand = pd.concat([rand_neg, positive_options]).sample(\n",
    "            frac=1, replace=False, random_state=seed\n",
    "        )  # add to pos and shuffle\n",
    "        rand.to_csv(out_dir + f\"random_seed_{seed}_N_{i}.csv\", index=False)\n",
    "\n",
    "        # Second way: most similar to positive\n",
    "        # Here, seeds only change the order of compounds in the df\n",
    "        set_of_sorted_similar_indices = sorted_indices_most_similar[0:i]\n",
    "        similar_neg = negative_options.iloc[set_of_sorted_similar_indices, :]\n",
    "        similar = pd.concat([similar_neg, positive_options]).sample(\n",
    "            frac=1, replace=False, random_state=seed\n",
    "        )  # add to pos and shuffle\n",
    "        similar.to_csv(out_dir + \"similar_seed_{seed}_N_{i}.csv\", index=False)\n",
    "\n",
    "        # Third way: most diverse - use  MaxMin algorithm\n",
    "        # (Ashton, M. et. al., Quant. Struct.-Act. Relat., 21 (2002), 598-604).\n",
    "        # http://rdkit.blogspot.com/2014/08/picking-diverse-compounds-from-large.html\n",
    "        # takes a ton of time for larger Ns\n",
    "        mmp = SimDivFilters.MaxMinPicker(seed=seed)\n",
    "        diverse_indices = mmp.LazyBitVectorPick(all_neg_fps, len(all_neg_fps), i)\n",
    "        diverse_neg = negative_options.iloc[diverse_indices, :]\n",
    "        diverse = pd.concat([diverse_neg, positive_options]).sample(frac=1)  # add to pos and shuffle\n",
    "        diverse.to_csv(out_dir + \"diverse_seed_{seed}_N_{i}.csv\", index=False)\n",
    "\n",
    "        # Now, let's prove to ourselves that we did this right by plotting one of the small examples\n",
    "        # Batch similarity calculation for faster computation\n",
    "\n",
    "        def calculate_pairwise_tanimoto(fps_indices, fps_list):\n",
    "            \"\"\"\n",
    "            Compute pairwise Tanimoto similarities for a selected subset of fingerprints.\n",
    "\n",
    "            Parameters:\n",
    "            fps_indices (iterable): Indices (from the original dataframe) of the fingerprints to compare.\n",
    "            fps_list (list): List of RDKit fingerprint objects for all negative molecules.\n",
    "\n",
    "            Returns:\n",
    "            np.ndarray: 1D array of pairwise Tanimoto similarities (upper triangle, no diagonal).\n",
    "            \"\"\"\n",
    "            # Convert dataframe indices to positional indices for alignment with fps_list\n",
    "            positional_indices = [negative_options.index.get_loc(idx) for idx in fps_indices]\n",
    "            selected_fps = [fps_list[idx] for idx in positional_indices]\n",
    "\n",
    "            similarity_matrix = np.zeros((len(selected_fps), len(selected_fps)))\n",
    "\n",
    "            for i in range(len(selected_fps)):\n",
    "                similarity_matrix[i, i:] = DataStructs.BulkTanimotoSimilarity(selected_fps[i], selected_fps[i:])\n",
    "\n",
    "            # Extract upper triangle of the matrix (excluding the diagonal)\n",
    "            pairwise_similarities = similarity_matrix[np.triu_indices_from(similarity_matrix, k=1)]\n",
    "            return pairwise_similarities\n",
    "\n",
    "        if i < 1000:  # plot to confirm selection is working as intended\n",
    "            # For random negatives\n",
    "            random_tanimoto_scores = calculate_pairwise_tanimoto(rand_neg.index, all_neg_fps)\n",
    "            plt.hist(random_tanimoto_scores, bins=30, label=\"random\", alpha=0.7)\n",
    "\n",
    "            # For similar negatives\n",
    "            similar_tanimoto_scores = calculate_pairwise_tanimoto(\n",
    "                negative_options.iloc[set_of_sorted_similar_indices].index, all_neg_fps\n",
    "            )\n",
    "            plt.hist(similar_tanimoto_scores, bins=30, label=\"similar\", alpha=0.7)\n",
    "\n",
    "            # For diverse negatives\n",
    "            diverse_tanimoto_scores = calculate_pairwise_tanimoto(\n",
    "                negative_options.iloc[diverse_indices].index, all_neg_fps\n",
    "            )\n",
    "            plt.hist(diverse_tanimoto_scores, bins=30, label=\"diverse\", alpha=0.7)\n",
    "\n",
    "            # Show plot\n",
    "            plt.legend()\n",
    "            plt.title(\"Pairwise Tanimoto Similarities\")\n",
    "            plt.xlabel(\"Tanimoto Similarity\")\n",
    "            plt.ylabel(\"Frequency\")\n",
    "            plt.tight_layout()\n",
    "            plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "977c2522",
   "metadata": {},
   "source": [
    "# Prep script to train models on these different datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16daffa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for seed in range(num_datasets_per_size):\n",
    "    for i in num_neg:\n",
    "        for j in [\"random\", \"similar\", \"diverse\"]:\n",
    "            # Skip non-random methods for the largest dataset\n",
    "            if i == len(df) - pos and j != \"random\":\n",
    "                continue\n",
    "\n",
    "            # Names for folders and paths\n",
    "            clean_name = f\"{j}_seed_{seed}_N_{i}\"\n",
    "            models_fold = f\"{models_dir}/{clean_name}/\"\n",
    "            curr_data_path = f\"{out_dir}/{clean_name}.csv\"\n",
    "\n",
    "            mk_folder_command = f\"mkdir -p {models_fold}\"  # Use -p to avoid errors if the folder exists\n",
    "            train_command = (\n",
    "                f\"chemprop_train --dropout 0.1 --hidden_size 500 --ffn_num_layers 2 --depth 3 \"\n",
    "                f\"--metric prc-auc --extra_metrics auc --save_dir {models_fold} --data_path {curr_data_path} \"\n",
    "                f\"--dataset_type classification --features_generator rdkit_2d_normalized --no_features_scaling \"\n",
    "                f\"--num_folds 5 --ensemble_size 2 --split_type scaffold_balanced --split_sizes 0.8 0.1 0.1 \"\n",
    "                f\"--smiles_columns SMILES --target_columns hit --gpu 0\"\n",
    "            )\n",
    "\n",
    "            # Write the commands to the bash script\n",
    "            with open(bash_dir, \"a\") as file1:\n",
    "                file1.write(mk_folder_command + \"; \" + train_command + \"\\n\")\n",
    "\n",
    "# Then, use script and train the models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "81f2adf7",
   "metadata": {},
   "source": [
    "# Use different neg dataset models to predict on 20% random test set + external val set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f58cef5",
   "metadata": {},
   "source": [
    "Note: The ../../chemprop/ path must exist and contain the predict.py script. We assume the relevant conda environment name is 'chemprop'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "purple-finnish",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "activate_command = \"conda activate chemprop; \"\n",
    "\n",
    "# num_neg = [508, 1017, 2034, 4068, 8136, 16272, 29950]\n",
    "for seed in range(num_datasets_per_size):\n",
    "    for i in num_neg:\n",
    "        for j in [\"random\", \"similar\", \"diverse\"]:\n",
    "            for data_name, data in zip(\n",
    "                [\"external_val\", \"20%_random_test\"],\n",
    "                [\n",
    "                    \"cleaned_easy_med_hard_val_sets_800k_10_03_2022.csv\",\n",
    "                    \"TEST_03_19_2022.csv\",\n",
    "                ],\n",
    "            ):\n",
    "\n",
    "                # Names for folders and paths\n",
    "                clean_name = f\"{j}_seed_{seed}_N_{i}\"\n",
    "                models_fold = f\"{models_dir}/{clean_name}/\"\n",
    "                curr_data_path = f\"{out_dir}/{clean_name}.csv\"\n",
    "                pred_path = f\"{out_dir}/pred_{data_name}_{clean_name}.csv\"\n",
    "\n",
    "                try:\n",
    "                    # Updated file path to reflect the new naming convention\n",
    "                    testdf = pd.read_csv(pred_path)\n",
    "                    continue  # already exists\n",
    "                except FileNotFoundError:\n",
    "                    print(clean_name, data)\n",
    "                    run_command = (\n",
    "                        f\"python predict.py --test_path {out_dir}/{data} \"\n",
    "                        f\"--checkpoint_dir {models_fold}/ --preds_path {pred_path} \"\n",
    "                        f\"--features_generator rdkit_2d_normalized --no_features_scaling --smiles_column SMILES \"\n",
    "                        f\"--ensemble_variance --gpu 0\"\n",
    "                    )\n",
    "                    full_command = activate_command + run_command\n",
    "                    test = subprocess.run(\n",
    "                        full_command,\n",
    "                        cwd=\"../../chemprop/\",\n",
    "                        shell=True,\n",
    "                        capture_output=True,\n",
    "                    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "87e1cc24",
   "metadata": {},
   "source": [
    "# Quantify predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1067edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modeleval(y_true, y_pred, name=\"\"):\n",
    "    \"\"\"\n",
    "    Evaluates model predictions using AUROC and Precision-Recall AUC.\n",
    "\n",
    "    Parameters:\n",
    "    y_true (list or array-like): Ground-truth binary labels (0/1).\n",
    "    y_pred (list or array-like): Predicted probabilities or scores (floats).\n",
    "    name (str, optional): Optional model name or identifier for logging. Default is \"\".\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing:\n",
    "        - float: AUROC score.\n",
    "        - float: Area under the Precision-Recall curve (AUPR).\n",
    "    \"\"\"\n",
    "    # Compute auROC\n",
    "    auroc = float(roc_auc_score(y_true, y_pred))\n",
    "\n",
    "    # Compute Precision-Recall\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_pred)\n",
    "    pr = float(auc(recall, precision))\n",
    "\n",
    "    return (auroc, pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bdecd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"Seed\", \"N (negatives)\", \"Data Selection\", \"Test Set\", \"Metric\", \"Value\"]\n",
    "results = pd.DataFrame(columns=columns)\n",
    "\n",
    "for seed in range(num_datasets_per_size):\n",
    "    for i in num_neg:\n",
    "        for j in [\"random\", \"similar\", \"diverse\"]:\n",
    "            for data_name, data in zip(\n",
    "                [\"external_val\", \"20%_random_test\"],\n",
    "                [\n",
    "                    \"cleaned_easy_med_hard_val_sets_800k_10_03_2022.csv\",\n",
    "                    \"TEST_03_19_2022.csv\",\n",
    "                ],\n",
    "            ):\n",
    "                clean_name = f\"{j}_seed_{seed}_N_{i}\"\n",
    "\n",
    "                true_path = f\"{out_dir}/{data}\"\n",
    "                pred_path = f\"{out_dir}/pred_{data_name}_{clean_name}.csv\"\n",
    "\n",
    "                try:\n",
    "                    true = pd.read_csv(true_path)\n",
    "                    true = list(true[\"hit\"])\n",
    "\n",
    "                    test = pd.read_csv(pred_path)\n",
    "                    test = list(test[\"hit\"])\n",
    "                except FileNotFoundError:\n",
    "                    continue\n",
    "\n",
    "                # Calculate metrics\n",
    "                roc, pr = modeleval(true, test)\n",
    "\n",
    "                # Prepare rows for results\n",
    "                row1 = [seed, i, j, data_name, \"auROC\", roc]\n",
    "                row2 = [seed, i, j, data_name, \"auPR\", pr]\n",
    "                new = pd.DataFrame([row1, row2], columns=columns)\n",
    "\n",
    "                # Append results\n",
    "                results = pd.concat([results, new], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e47ea41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename for plotting\n",
    "results[\"Test Set\"] = [\n",
    "    \"Restricted Similarity\" if x == \"external_val\" else \"Random (20%)\" for x in list(results[\"Test Set\"])\n",
    "]\n",
    "results.to_csv(f\"{fig_dir}_RESULTS_SEEDED.csv\", index=False)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885a4b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = results.reset_index(drop=True)\n",
    "order = [\"random\", \"similar\", \"diverse\"]\n",
    "palette = dict(zip(order, [\"grey\", \"lightsteelblue\", \"royalblue\"]))\n",
    "sns.set(rc={\"figure.dpi\": 300, \"savefig.dpi\": 300})\n",
    "sns.set_style(\"whitegrid\", {\"axes.grid\": False})\n",
    "g = sns.FacetGrid(\n",
    "    results,\n",
    "    row=\"Metric\",\n",
    "    col=\"Test Set\",\n",
    "    sharey=True,\n",
    "    height=4,\n",
    "    aspect=1.5,\n",
    "    margin_titles=True,\n",
    ")\n",
    "g.map(\n",
    "    sns.lineplot,\n",
    "    \"N (negatives)\",\n",
    "    \"Value\",\n",
    "    \"Data Selection\",\n",
    "    marker=\"o\",\n",
    "    dashes=True,\n",
    "    hue_order=order,\n",
    "    palette=palette,\n",
    ")\n",
    "\n",
    "for ax in g.axes_dict.values():\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "g.set(xscale=\"log\")\n",
    "g.set(xticks=num_neg)\n",
    "g.set(xticklabels=num_neg)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(fig_dir + \".png\")\n",
    "plt.savefig(fig_dir + \".svg\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chemprop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
