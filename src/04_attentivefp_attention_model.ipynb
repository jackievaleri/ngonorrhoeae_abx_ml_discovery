{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1072b2dd",
   "metadata": {},
   "source": [
    "# Installation instructions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `conda create --name=attentivefp_env python=3.8`\n",
    "- `conda activate attentivefp_env`\n",
    "- `pip install deepchem`\n",
    "- `pip install dgl dgllife`\n",
    "- `pip install torch`\n",
    "- `pip install numpy==1.21.1`\n",
    "- `conda deactivate`\n",
    "- `python -m ipykernel install --user --name=attentivefp_env`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "import deepchem as dc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from deepchem.models import AttentiveFPModel\n",
    "from hyperopt import Trials, fmin, hp, tpe\n",
    "from sklearn.metrics import auc, precision_recall_curve, roc_auc_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:32:41] Unusual charge on atom 0 number of radical electrons set to zero\n",
      "Failed to featurize datapoint 4247, Cl.[NaH]. Appending empty array\n",
      "Exception message: tuple index out of range\n",
      "/home/jackievaleri8/.conda/envs/attentivefp_env/lib/python3.8/site-packages/deepchem/feat/base_classes.py:323: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.asarray(features)\n",
      "Failed to featurize datapoint 1543, Cl.[LiH]. Appending empty array\n",
      "Exception message: tuple index out of range\n",
      "/home/jackievaleri8/.conda/envs/attentivefp_env/lib/python3.8/site-packages/deepchem/feat/base_classes.py:323: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.asarray(features)\n",
      "Failed to featurize datapoint 7461, I.[KH]. Appending empty array\n",
      "Exception message: tuple index out of range\n",
      "/home/jackievaleri8/.conda/envs/attentivefp_env/lib/python3.8/site-packages/deepchem/feat/base_classes.py:323: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.asarray(features)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<DiskDataset X.shape: (30964,), y.shape: (30964, 1), w.shape: (30964, 1), task_names: ['hit']>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featurizer = dc.feat.MolGraphConvFeaturizer(use_edges=True)\n",
    "\n",
    "tasks = [\"hit\"]\n",
    "input_file = \"../data/TRAIN_03_19_2022.csv\"\n",
    "loader = dc.data.CSVLoader(tasks=tasks, feature_field=\"SMILES\", featurizer=featurizer)\n",
    "\n",
    "dataset = loader.create_dataset(input_file)\n",
    "dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model - do hyperparameter search on 80% train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "NUM_FOLDS = 3\n",
    "SAVE_INTERVAL = 100\n",
    "NB_EPOCH = 10\n",
    "OPT_TRIALS = 10\n",
    "BATCH_SIZE = 50\n",
    "\n",
    "# hyperparameter optimization\n",
    "search_space = {\n",
    "    \"num_layers\": hp.choice(\"num_layers\", [1, 2, 3, 4]),\n",
    "    \"layer_sizes\": hp.choice(\"layer_sizes\", [200, 400, 600, 800, 1000]),  # graph_feat_size\n",
    "    \"dropout\": hp.uniform(\"dropout\", low=0.1, high=0.4),\n",
    "    \"learning_rate\": hp.uniform(\"learning_rate\", high=0.001, low=0.00001),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_attentive_fp_model(save_dir, num_layers, learning_rate=0.001, dropout=0.1, graph_feat_size=200):\n",
    "    \"\"\"\n",
    "    Trains an AttentiveFP graph neural network model using scaffold-based data splitting.\n",
    "\n",
    "    The model is trained across multiple random seeds (NUM_FOLDS) with early stopping\n",
    "    on validation performance. For each fold, auPR and auROC are computed on the test set,\n",
    "    results are saved to disk, and averages are reported.\n",
    "\n",
    "    Parameters:\n",
    "    save_dir (str): Directory where fold results and models will be saved.\n",
    "    num_layers (int): Number of graph convolution layers in the AttentiveFP model.\n",
    "    learning_rate (float, optional): Learning rate for model optimization. Default is 0.001.\n",
    "    dropout (float, optional): Dropout probability for regularization. Default is 0.1.\n",
    "    graph_feat_size (int, optional): Dimensionality of learned graph feature vectors. Default is 200.\n",
    "\n",
    "    Returns:\n",
    "    float: Average auPR across all folds.\n",
    "    \n",
    "    Notes:\n",
    "    - Requires global variables: NUM_FOLDS, BATCH_SIZE, SAVE_INTERVAL, NB_EPOCH, and dataset.\n",
    "    - Writes per-seed and average results to text files in save_dir.\n",
    "    - Uses DeepChem's ScaffoldSplitter for train/valid/test partitioning.\n",
    "    \"\"\"\n",
    "    metrics = [\n",
    "        dc.metrics.Metric(dc.metrics.roc_auc_score),\n",
    "        dc.metrics.Metric(dc.metrics.prc_auc_score),\n",
    "    ]\n",
    "\n",
    "    print(\"Saving results to \" + save_dir)\n",
    "    # Train the model with scaffold splitting\n",
    "    auprs = []\n",
    "    aurocs = []\n",
    "    df = open(save_dir + \"results.txt\", \"w\")\n",
    "    for seed in list(range(NUM_FOLDS)):\n",
    "        print(\"Seed: \" + str(seed))\n",
    "        model_dir = save_dir + str(seed) + \"/\"\n",
    "        os.mkdir(model_dir)\n",
    "\n",
    "        splitter = dc.splits.ScaffoldSplitter()\n",
    "        model = AttentiveFPModel(\n",
    "            model_dir=model_dir,\n",
    "            mode=\"classification\",\n",
    "            n_classes=2,\n",
    "            n_tasks=1,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            num_layers=num_layers,\n",
    "            learning_rate=learning_rate,\n",
    "            dropout=dropout,\n",
    "            graph_feat_size=graph_feat_size,\n",
    "        )\n",
    "        train, valid, test = splitter.train_valid_test_split(\n",
    "            dataset, frac_train=0.8, frac_valid=0.1, frac_test=0.1, seed=seed\n",
    "        )\n",
    "\n",
    "        # this automatically enables EarlyStopping based on validation metrics, and saves final model to file\n",
    "        tr = open(model_dir + str(seed) + \".txt\", \"w\")\n",
    "        vc = dc.models.ValidationCallback(\n",
    "            valid,\n",
    "            interval=SAVE_INTERVAL,\n",
    "            metrics=metrics,\n",
    "            save_dir=model_dir,\n",
    "            output_file=tr,\n",
    "        )\n",
    "        model.fit(train, nb_epoch=NB_EPOCH, checkpoint_interval=SAVE_INTERVAL, callbacks=[vc])\n",
    "\n",
    "        # Evaluate the model - auPR on test set\n",
    "        model.restore(model_dir=model_dir)  # restoring the best checkpoint\n",
    "        met = model.evaluate(test, metrics)\n",
    "        aupr = met[\"prc_auc_score\"]\n",
    "        auroc = met[\"roc_auc_score\"]\n",
    "\n",
    "        # save results\n",
    "        df.write(\"auPR on test set for seed \" + str(seed) + \": \" + str(aupr) + \"\\n\")\n",
    "        df.write(\"auROC on test set for seed \" + str(seed) + \": \" + str(auroc) + \"\\n\")\n",
    "        auprs.append(aupr)\n",
    "        aurocs.append(auroc)\n",
    "\n",
    "    df.close()\n",
    "    avg_aupr = np.mean(auprs)\n",
    "    print(\"Average auPR: \" + str(avg_aupr))\n",
    "    print(\"Average auROC: \" + str(np.mean(aurocs)))\n",
    "    return avg_aupr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the metrics\n",
    "\n",
    "\n",
    "def fm(args):\n",
    "    \"\"\"\n",
    "    Wrapper function for training an AttentiveFP model with hyperparameter inputs.\n",
    "\n",
    "    Constructs a save directory based on the provided hyperparameters, trains an\n",
    "    AttentiveFP model using `train_attentive_fp_model`, and returns the negative\n",
    "    auPR score (suitable for minimization in hyperparameter optimization).\n",
    "\n",
    "    Parameters:\n",
    "    args (dict): Dictionary of hyperparameters with keys:\n",
    "        - \"num_layers\" (int): Number of graph convolution layers.\n",
    "        - \"learning_rate\" (float): Learning rate for model training.\n",
    "        - \"dropout\" (float): Dropout probability.\n",
    "        - \"layer_sizes\" (int): Size of graph feature vectors.\n",
    "\n",
    "    Returns:\n",
    "    float: Negative auPR value on the test set (so optimizers can minimize).\n",
    "    \n",
    "    Side Effects:\n",
    "    - Creates a save directory for each hyperparameter combination.\n",
    "    - Prints fold identifier and auPR to stdout.\n",
    "    - Saves trained model and results in the created directory.\n",
    "    \"\"\"\n",
    "    nl = args[\"num_layers\"]\n",
    "    lr = args[\"learning_rate\"]\n",
    "    dr = args[\"dropout\"]\n",
    "    ls = args[\"layer_sizes\"]\n",
    "    fold = str(nl) + \"_\" + str(lr) + \"_\" + str(dr) + \"_\" + str(ls) + \"/\"\n",
    "    save_dir = \"../models/attentiveFP/hyperopt/\" + fold\n",
    "    os.mkdir(save_dir)\n",
    "    pr_auc = train_attentive_fp_model(save_dir, num_layers=nl, learning_rate=lr, dropout=dr, graph_feat_size=ls)\n",
    "    print(fold, pr_auc)\n",
    "    return -1 * pr_auc  # need to give something to MINIMIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = Trials()\n",
    "best = fmin(fm, space=search_space, algo=tpe.suggest, max_evals=OPT_TRIALS, trials=trials)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate best model on 20% test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>NL</th>\n",
       "      <th>LR</th>\n",
       "      <th>DR</th>\n",
       "      <th>LS</th>\n",
       "      <th>auPR</th>\n",
       "      <th>auROC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3_0.00016298505613644663_0.3254185003397073_1000</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>0.325419</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.299143</td>\n",
       "      <td>0.733583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4_0.00024127546611118732_0.1477493201861702_1000</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000241</td>\n",
       "      <td>0.147749</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.286854</td>\n",
       "      <td>0.737010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2_0.0006961750403834422_0.3813669623919579_200</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000696</td>\n",
       "      <td>0.381367</td>\n",
       "      <td>200</td>\n",
       "      <td>0.279951</td>\n",
       "      <td>0.729365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4_0.0004397299825071773_0.35543274010899906_800</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000440</td>\n",
       "      <td>0.355433</td>\n",
       "      <td>800</td>\n",
       "      <td>0.279225</td>\n",
       "      <td>0.724282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2_0.0006729163068408161_0.39667630722646385_200</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000673</td>\n",
       "      <td>0.396676</td>\n",
       "      <td>200</td>\n",
       "      <td>0.276575</td>\n",
       "      <td>0.727486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2_0.00010317567681770247_0.19535478942063683_600</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000103</td>\n",
       "      <td>0.195355</td>\n",
       "      <td>600</td>\n",
       "      <td>0.258253</td>\n",
       "      <td>0.717409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3_8.52739668146465e-05_0.13598456595554664_400</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.135985</td>\n",
       "      <td>400</td>\n",
       "      <td>0.249204</td>\n",
       "      <td>0.732744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4_0.00012105200153024894_0.3450562161243306_400</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.345056</td>\n",
       "      <td>400</td>\n",
       "      <td>0.241298</td>\n",
       "      <td>0.716436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4_0.0006288632466732576_0.2674342530096203_1000</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000629</td>\n",
       "      <td>0.267434</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.222929</td>\n",
       "      <td>0.655960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1_0.0008797286773333985_0.29080578548327574_1000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000880</td>\n",
       "      <td>0.290806</td>\n",
       "      <td>1000</td>\n",
       "      <td>0.180468</td>\n",
       "      <td>0.596180</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               name  NL        LR        DR  \\\n",
       "9  3_0.00016298505613644663_0.3254185003397073_1000   3  0.000163  0.325419   \n",
       "5  4_0.00024127546611118732_0.1477493201861702_1000   4  0.000241  0.147749   \n",
       "8    2_0.0006961750403834422_0.3813669623919579_200   2  0.000696  0.381367   \n",
       "2   4_0.0004397299825071773_0.35543274010899906_800   4  0.000440  0.355433   \n",
       "3   2_0.0006729163068408161_0.39667630722646385_200   2  0.000673  0.396676   \n",
       "6  2_0.00010317567681770247_0.19535478942063683_600   2  0.000103  0.195355   \n",
       "7    3_8.52739668146465e-05_0.13598456595554664_400   3  0.000085  0.135985   \n",
       "1   4_0.00012105200153024894_0.3450562161243306_400   4  0.000121  0.345056   \n",
       "0   4_0.0006288632466732576_0.2674342530096203_1000   4  0.000629  0.267434   \n",
       "4  1_0.0008797286773333985_0.29080578548327574_1000   1  0.000880  0.290806   \n",
       "\n",
       "     LS      auPR     auROC  \n",
       "9  1000  0.299143  0.733583  \n",
       "5  1000  0.286854  0.737010  \n",
       "8   200  0.279951  0.729365  \n",
       "2   800  0.279225  0.724282  \n",
       "3   200  0.276575  0.727486  \n",
       "6   600  0.258253  0.717409  \n",
       "7   400  0.249204  0.732744  \n",
       "1   400  0.241298  0.716436  \n",
       "0  1000  0.222929  0.655960  \n",
       "4  1000  0.180468  0.596180  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_param_name(params):\n",
    "    \"\"\"\n",
    "    Parses a parameter string into individual hyperparameters for AttentiveFP models.\n",
    "\n",
    "    Expects a string in the format: \n",
    "        \"<num_layers>_<learning_rate>_<dropout>_<layer_size>/\"\n",
    "\n",
    "    Parameters:\n",
    "    params (str): Parameter string encoding model hyperparameters.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing:\n",
    "        - int: Number of layers (nl).\n",
    "        - float: Learning rate (lr).\n",
    "        - float: Dropout probability (dr).\n",
    "        - int: Graph feature vector size (ls).\n",
    "    \"\"\"\n",
    "    nl = int(params.split(\"_\")[0])\n",
    "    lr = float(params.split(\"_\")[1].split(\"_\")[0])\n",
    "    dr = float(params.split(\"_\")[2].split(\"_\")[0])\n",
    "    ls = int(params.split(\"_\")[3].split(\"/\")[0])\n",
    "    return (nl, lr, dr, ls)\n",
    "\n",
    "\n",
    "# get best model\n",
    "path = \"../models/attentiveFP/hyperopt/\"\n",
    "columns = [\"name\", \"NL\", \"LR\", \"DR\", \"LS\", \"auPR\", \"auROC\"]\n",
    "ho_df = pd.DataFrame(columns=columns)\n",
    "for name in glob.glob(path + \"*/results.txt\"):\n",
    "    clean_name = name.split(\"/results.txt\")[0].split(\"hyperopt/\")[1]\n",
    "    results = pd.read_csv(name, header=None)\n",
    "    auprs = []\n",
    "    aurocs = []\n",
    "    for i, row in results.iterrows():\n",
    "        r = str(row)\n",
    "        val = float(r.split(\":\")[1].split(\"\\n\")[0])\n",
    "        if \"auPR\" in r:\n",
    "            auprs.append(val)\n",
    "        else:\n",
    "            aurocs.append(val)\n",
    "    nl, lr, dr, ls = split_param_name(clean_name)\n",
    "    ho_df.loc[len(ho_df.index)] = [\n",
    "        clean_name,\n",
    "        nl,\n",
    "        lr,\n",
    "        dr,\n",
    "        ls,\n",
    "        np.mean(auprs),\n",
    "        np.mean(aurocs),\n",
    "    ]\n",
    "ho_df = ho_df.sort_values(\"auPR\", ascending=False)\n",
    "ho_df.to_csv(path + \"all_ho_results.csv\", index=False)\n",
    "ho_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DiskDataset X.shape: (7713,), y.shape: (7713, 1), w.shape: (7713, 1), task_names: ['hit']>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prep test set\n",
    "featurizer = dc.feat.MolGraphConvFeaturizer(use_edges=True)\n",
    "\n",
    "tasks = [\"hit\"]\n",
    "test_input_file = \"../data/TEST_03_19_2022.csv\"\n",
    "loader = dc.data.CSVLoader(tasks=tasks, feature_field=\"SMILES\", featurizer=featurizer)\n",
    "\n",
    "held_out_test = loader.create_dataset(test_input_file)\n",
    "held_out_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'roc_auc_score': 0.8536963639496812, 'prc_auc_score': 0.286347165353454}\n",
      "{'roc_auc_score': 0.8611104462783192, 'prc_auc_score': 0.23837542752904364}\n",
      "{'roc_auc_score': 0.8550949062107084, 'prc_auc_score': 0.2385725525123126}\n",
      "auPR on held-out 20% test set: 0.2738796075580747\n",
      "auROC on held-out 20% test set: 0.8635866824950058\n"
     ]
    }
   ],
   "source": [
    "params = \"3_0.00016298505613644663_0.3254185003397073_1000/\"\n",
    "preds = pd.DataFrame()\n",
    "metrics = [\n",
    "    dc.metrics.Metric(dc.metrics.roc_auc_score),\n",
    "    dc.metrics.Metric(dc.metrics.prc_auc_score),\n",
    "]\n",
    "for seed in list(range(NUM_FOLDS)):\n",
    "    nl, lr, dr, ls = split_param_name(params)\n",
    "    correct_dir = path + params + str(seed) + \"/\"\n",
    "    model = AttentiveFPModel(\n",
    "        model_dir=correct_dir,\n",
    "        mode=\"classification\",\n",
    "        n_tasks=1,\n",
    "        n_classes=2,\n",
    "        num_layers=nl,\n",
    "        learning_rate=lr,\n",
    "        dropout=dr,\n",
    "        graph_feat_size=ls,\n",
    "    )\n",
    "    model.restore(model_dir=correct_dir)\n",
    "    met = model.evaluate(held_out_test, metrics)\n",
    "    print(met)  # this is the predictions when using 1 fold for prediction\n",
    "\n",
    "    # actually save the predictions so we can average them - use the ensemble for prediction\n",
    "    y_pred = model.predict(held_out_test)\n",
    "    y_pred = y_pred[:, 1]\n",
    "    preds[str(seed)] = list(y_pred)\n",
    "\n",
    "avg_preds = preds.mean(axis=1)\n",
    "preds[\"mean\"] = avg_preds\n",
    "preds.to_csv(path + params + \"predictions.csv\", index=False)\n",
    "\n",
    "y_true = held_out_test.y\n",
    "y_true = y_true.flatten()\n",
    "precision, recall, threshold = precision_recall_curve(y_true, avg_preds)\n",
    "pr_auc = auc(recall, precision)\n",
    "roc = roc_auc_score(y_true, avg_preds)\n",
    "\n",
    "print(\"auPR on held-out 20% test set: \" + str(pr_auc))\n",
    "print(\"auROC on held-out 20% test set: \" + str(roc))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to featurize datapoint 4247, Cl.[NaH]. Appending empty array\n",
      "Exception message: tuple index out of range\n",
      "/home/jackievaleri8/.conda/envs/attentivefp_env/lib/python3.8/site-packages/deepchem/feat/base_classes.py:323: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.asarray(features)\n",
      "Failed to featurize datapoint 1543, Cl.[LiH]. Appending empty array\n",
      "Exception message: tuple index out of range\n",
      "/home/jackievaleri8/.conda/envs/attentivefp_env/lib/python3.8/site-packages/deepchem/feat/base_classes.py:323: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.asarray(features)\n",
      "Failed to featurize datapoint 7461, I.[KH]. Appending empty array\n",
      "Exception message: tuple index out of range\n",
      "/home/jackievaleri8/.conda/envs/attentivefp_env/lib/python3.8/site-packages/deepchem/feat/base_classes.py:323: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.asarray(features)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<DiskDataset X.shape: (38677,), y.shape: (38677, 1), w.shape: (38677, 1), task_names: ['hit']>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featurizer = dc.feat.MolGraphConvFeaturizer(use_edges=True)\n",
    "\n",
    "tasks = [\"hit\"]\n",
    "input_file = \"../data/FULL_03_19_2022.csv\"\n",
    "loader = dc.data.CSVLoader(tasks=tasks, feature_field=\"SMILES\", featurizer=featurizer)\n",
    "\n",
    "dataset = loader.create_dataset(input_file)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"../models/attentiveFP/FINAL_random_split_3_0.00016298505613644663_0.3254185003397073_1000/\"\n",
    "os.mkdir(save_dir)\n",
    "print(\"Saving results to \" + save_dir)\n",
    "\n",
    "NUM_FOLDS = 20\n",
    "NUM_LAYERS = 3\n",
    "LEARNING_RATE = 0.00016298505613644663\n",
    "DROPOUT = 0.3254185003397073\n",
    "GRAPH_FEAT_SIZE = 1000\n",
    "\n",
    "# Train the model with best parameters\n",
    "auprs = []\n",
    "aurocs = []\n",
    "metrics = [\n",
    "    dc.metrics.Metric(dc.metrics.roc_auc_score),\n",
    "    dc.metrics.Metric(dc.metrics.prc_auc_score),\n",
    "]\n",
    "for seed in list(range(NUM_FOLDS)):\n",
    "    print(\"Seed: \" + str(seed))\n",
    "    model_dir = save_dir + str(seed) + \"/\"\n",
    "    os.mkdir(model_dir)\n",
    "\n",
    "    splitter = dc.splits.RandomSplitter()  # dc.splits.ScaffoldSplitter()\n",
    "    model = AttentiveFPModel(\n",
    "        model_dir=model_dir,\n",
    "        mode=\"classification\",\n",
    "        n_classes=2,\n",
    "        n_tasks=1,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_layers=NUM_LAYERS,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        dropout=DROPOUT,\n",
    "        graph_feat_size=GRAPH_FEAT_SIZE,\n",
    "    )\n",
    "    train, valid, test = splitter.train_valid_test_split(\n",
    "        dataset, frac_train=0.8, frac_valid=0.1, frac_test=0.1, seed=seed\n",
    "    )\n",
    "\n",
    "    # this automatically enables EarlyStopping based on validation metrics, and saves final model to file\n",
    "    tr = open(model_dir + str(seed) + \".txt\", \"w\")\n",
    "    vc = dc.models.ValidationCallback(\n",
    "        valid,\n",
    "        interval=SAVE_INTERVAL,\n",
    "        metrics=metrics,\n",
    "        save_dir=model_dir,\n",
    "        output_file=tr,\n",
    "    )\n",
    "    model.fit(train, nb_epoch=NB_EPOCH, checkpoint_interval=SAVE_INTERVAL, callbacks=[vc])\n",
    "\n",
    "    # Evaluate the model - actually need to save predictions so we can average them\n",
    "    model.restore(model_dir=model_dir)  # restoring the best checkpoint\n",
    "    met = model.evaluate(test, metrics)\n",
    "    aupr = met[\"prc_auc_score\"]\n",
    "    auroc = met[\"roc_auc_score\"]\n",
    "\n",
    "    # save results\n",
    "    print(\"auPR on test set for seed \" + str(seed) + \": \" + str(aupr))\n",
    "    print(\"auROC on test set for seed \" + str(seed) + \": \" + str(auroc))\n",
    "    auprs.append(aupr)\n",
    "    aurocs.append(auroc)\n",
    "\n",
    "avg_aupr = np.mean(auprs)\n",
    "print(\"Average auPR: \" + str(avg_aupr))\n",
    "avg_auroc = np.mean(aurocs)\n",
    "print(\"Average auROC: \" + str(avg_aupr))\n",
    "\n",
    "results_df = pd.DataFrame()\n",
    "results_df[\"auPR\"] = auprs\n",
    "results_df[\"auROC\"] = aurocs\n",
    "results_df.to_csv(save_dir + \"summary_results.csv\", index=False)\n",
    "results_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "attentivefp_env",
   "language": "python",
   "name": "attentivefp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
